(spacy) D:\GitHub\Spacy-Serbian-Transformer>python -m spacy train config7.cfg --output ./model7 --gpu-id 0
ℹ Saving to output directory: model7
ℹ Using GPU: 0

=========================== Initializing pipeline ===========================
Some weights of the model checkpoint at bertovic-base2 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at bertovic-base2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✔ Initialized pipeline

============================= Training pipeline =============================
ℹ Pipeline: ['transformer', 'morphologizer', 'trainable_lemmatizer',
'senter']
ℹ Set annotations on update for: ['morphologizer']
ℹ Initial learn rate: 0.0
E    #       LOSS TRANS...  LOSS MORPH...  LOSS TRAIN...  LOSS SENTER  POS_ACC  MORPH_ACC  LEMMA_ACC  SENTS_F  SENTS_P  SENTS_R  SCORE
---  ------  -------------  -------------  -------------  -----------  -------  ---------  ---------  -------  -------  -------  ------
  0       0           0.00         739.69         788.28       394.50     1.85       0.00      47.89     0.00     0.00     0.00    0.16
  1     200        1472.25      320892.92      357562.39     80603.90    32.54       0.00      47.59     0.00     0.00     0.00    0.21
  3     400        5797.51      121316.06      226457.41     14415.40    95.63       0.00      65.62    75.09    78.03    72.37    0.63
  4     600        4714.00       16350.51       89355.84      4192.01    97.97       0.00      88.68    81.84    85.26    78.69    0.73
  6     800        4008.23        7712.00       37918.10      2078.33    98.10       0.00      92.25    87.02    87.23    86.82    0.76
  7    1000        3357.16        5309.68       24175.34      1369.14    98.14       0.00      93.65    87.58    87.75    87.42    0.76
  9    1200        2411.09        3340.09       16103.75       805.46    98.19       0.00      94.59    87.12    89.03    85.30    0.77
 11    1400        1939.45        2356.07       11830.61       630.03    98.20       0.00      95.27    87.53    89.80    85.38    0.77
 12    1600        1423.42        1439.29        8737.99       416.23    98.24       0.00      95.52    88.79    90.17    87.46    0.78
 14    1800        1210.70        1082.37        6640.15       392.46    98.15       0.00      95.84    88.62    88.81    88.43    0.78
 15    2000         934.66         838.96        4910.39       220.68    98.28       0.00      96.07    88.65    89.12    88.18    0.78
 17    2200         849.20         759.94        3703.19       161.99    98.22       0.00      95.96    89.71    90.19    89.24    0.78
 19    2400         769.86         583.62        2748.39       178.32    98.32       0.00      96.19    89.18    91.34    87.12    0.78
 20    2600         755.34         552.10        1948.56       196.76    98.26       0.00      96.22    89.98    90.52    89.45    0.78
 22    2800         699.44         439.33        1428.02       180.68    98.28       0.00      96.27    90.00    90.35    89.66    0.78
 23    3000         711.46         500.62        1066.37       127.26    98.26       0.00      96.26    88.69    90.74    86.74    0.78
 25    3200         661.69         447.69         770.33       155.94    98.26       0.00      96.28    89.73    90.59    88.90    0.78
 27    3400         583.61         391.60         557.15       142.28    98.19       0.00      96.29    89.75    90.70    88.81    0.78
 28    3600         547.55         356.76         515.42       103.73    98.28       0.00      96.32    89.53    90.48    88.60    0.78
 30    3800         689.55         391.75         344.06       169.40    98.28       0.00      96.31    89.54    90.62    88.47    0.78
 31    4000         498.94         314.21         338.80       106.64    98.28       0.00      96.34    89.56    90.85    88.31    0.78
 33    4200         547.47         286.53         308.34       125.45    98.27       0.00      96.35    89.18    89.82    88.56    0.78
 34    4400         546.73         315.03         307.92        96.88    98.29       0.00      96.36    90.28    90.82    89.75    0.78
 36    4600         491.26         305.46         237.58        89.62    98.25       0.00      96.34    89.37    91.15    87.67    0.78
 38    4800         458.01         282.91         256.21        74.29    98.25       0.00      96.31    89.32    90.62    88.05    0.78
 39    5000         470.83         277.73         265.39        88.24    98.28       0.00      96.33    90.05    90.54    89.58    0.78
 41    5200         400.18         301.38         208.99        62.19    98.31       0.00      96.37    90.88    91.21    90.55    0.79
 42    5400         452.88         289.06         213.69        71.82    98.28       0.00      96.35    90.34    91.12    89.58    0.78
 44    5600         391.90         247.49         195.08        75.60    98.29       0.00      96.36    89.79    91.10    88.52    0.78
 46    5800         394.71         222.43         216.47        67.99    98.31       0.00      96.37    89.20    91.19    87.29    0.78
 47    6000         426.22         236.77         183.50        89.98    98.30       0.00      96.36    90.13    90.69    89.58    0.78
 49    6200         396.58         235.87         178.27        66.28    98.21       0.00      96.32    89.73    91.20    88.31    0.78
 50    6400         349.20         247.56         169.72        50.37    98.26       0.00      96.33    89.13    91.14    87.20    0.78
 52    6600         353.32         229.27         173.24        55.83    98.25       0.00      96.36    90.74    91.40    90.08    0.78
 54    6800         323.69         225.01         158.21        50.76    98.23       0.00      96.33    91.10    91.75    90.47    0.79
 55    7000         328.42         202.06         147.10        54.31    98.34       0.00      96.40    89.68    90.61    88.77    0.78
 57    7200         311.47         202.98         141.17        50.02    98.34       0.00      96.47    89.62    91.74    87.58    0.78
 58    7400         306.74         200.95         145.70        44.22    98.35       0.00      96.44    90.49    91.29    89.70    0.78
 60    7600         276.42         188.80         134.88        52.24    98.28       0.00      96.39    90.74    91.40    90.08    0.78
 61    7800         387.66         218.04         138.19        70.74    98.32       0.00      96.42    90.03    90.57    89.49    0.78
 63    8000         302.17         185.93         123.63        53.59    98.29       0.00      96.38    90.45    91.43    89.49    0.78
 65    8200         262.16         160.75         111.84        60.60    98.28       0.00      96.39    90.59    91.89    89.32    0.78
 66    8400         275.19         183.72         115.90        56.58    98.30       0.00      96.46    89.64    91.19    88.14    0.78
✔ Saved pipeline to output directory
model7\model-last

(spacy) D:\GitHub\Spacy-Serbian-Transformer>python -m spacy benchmark accuracy ./model7/model-best ./Corpus/sr_set-ud-test.spacy --output ./eval_results7.json --gpu-id 0
ℹ Using GPU: 0

================================== Results ==================================

TOK      100.00
POS      90.98
MORPH    19.77
LEMMA    86.68
SENT P   79.62
SENT R   88.65
SENT F   83.89
SPEED    2897


============================== MORPH (per feat) ==============================

                  P      R      F
Case           0.00   0.00   0.00
Gender         0.00   0.00   0.00
Number         0.00   0.00   0.00
Tense          0.00   0.00   0.00
VerbForm       0.00   0.00   0.00
Voice          0.00   0.00   0.00
Animacy        0.00   0.00   0.00
Definite       0.00   0.00   0.00
Degree         0.00   0.00   0.00
Mood           0.00   0.00   0.00
Person         0.00   0.00   0.00
PronType       0.00   0.00   0.00
Polarity       0.00   0.00   0.00
Poss           0.00   0.00   0.00
Reflex         0.00   0.00   0.00
Number[psor]   0.00   0.00   0.00
NumType        0.00   0.00   0.00
Foreign        0.00   0.00   0.00
Gender[psor]   0.00   0.00   0.00

✔ Saved results to eval_results7.json

(spacy) D:\GitHub\Spacy-Serbian-Transformer>python -m spacy benchmark accuracy ./model7/model-best ./Corpus/SrpKor4Tagging-test.spacy --output ./eval_results7-1.json --gpu-id 0
ℹ Using GPU: 0

================================== Results ==================================

TOK      100.00
POS      98.22
MORPH    -
LEMMA    96.65
SENT P   90.11
SENT R   90.30
SENT F   90.20
SPEED    7347

✔ Saved results to eval_results7-1.json